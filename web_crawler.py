import asyncio
import aiofiles
import os
from urllib.parse import urlparse, parse_qs
from crawl4ai import AsyncWebCrawler
from numpy import True_
from playwright.async_api import async_playwright
from readability import Document
import html2text


# === é…ç½® ===
BASE_DIR = os.path.dirname(__file__)
HTML_FILE = os.path.join(BASE_DIR, "test/output.html")
MD_FILE = os.path.join(BASE_DIR, "test/output.md")
SH_PICTURE_FILE = os.path.join(BASE_DIR, "test/debug_screenshot.png")
WAIT_TIME = 15  # å¯è§†åŒ–æ¨¡å¼æ‰‹åŠ¨æ“ä½œç­‰å¾…æ—¶é—´
# VPN ä»£ç†é…ç½®
PROXIES = {
    'http': 'http://127.0.0.1:7890',
    'https': 'http://127.0.0.1:7890'
}


def resolve_google_redirect(url):
    """è§£æ Google è·³è½¬ URL"""
    parsed = urlparse(url)
    if "google.com" in parsed.netloc and "url" in parsed.path:
        query = parse_qs(parsed.query)
        if "url" in query:
            return query["url"][0]
    return url


def is_challenge_page(html):
    """åˆ¤æ–­æ˜¯å¦æ˜¯åçˆ¬/éªŒè¯ç é¡µé¢"""
    return any(
        keyword in html.lower()
        for keyword in ["javascript is disabled", "captcha", "cloudflare", "awswaf"]
    )


def extract_readable_content(html):
    if html:
        """ç”¨ Readability æå–æ­£æ–‡å¹¶è½¬ Markdown"""
        doc = Document(html)
        title = doc.short_title()
        content_html = doc.summary(html_partial=True)

        md_converter = html2text.HTML2Text()
        md_converter.ignore_links = False
        md_text = md_converter.handle(content_html)
        return title, content_html, md_text
    else:
        return None, None, ''


async def save_result(title, html, md_text, save_files=False):
    if not save_files:
        return
    """ä¿å­˜ HTML å’Œ Markdown"""
    async with aiofiles.open(HTML_FILE, "w", encoding="utf-8") as f:
        await f.write(html)
    async with aiofiles.open(MD_FILE, "w", encoding="utf-8") as f:
        await f.write(f"# {title}\n\n{md_text}")
    print(f"âœ… HTML ä¿å­˜åˆ°: {HTML_FILE}")
    print(f"âœ… Markdown ä¿å­˜åˆ°: {MD_FILE}")


async def try_crawl4ai(url):
    """ç¬¬ä¸€é˜¶æ®µï¼šç›´æ¥ç”¨ crawl4ai æŠ“å–"""
    try:
        async with AsyncWebCrawler(proxy=PROXIES['http'], max_concurrency=5) as crawler:
            result = await crawler.arun(url=url)
            if result and result.html and not is_challenge_page(result.html):
                return result.html
    except Exception as e:
        print(f"[crawl4ai æŠ“å–å¤±è´¥] {e}")
    return None


async def try_playwright(url, headless=True):
    """ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ Playwright æŠ“å–"""
    async with async_playwright() as p:
        try:
            browser = await p.chromium.launch(
                headless=headless, 
                slow_mo=100,
                proxy={
                    "server": PROXIES['http'],
                }
            )
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                # æ·»åŠ æŒä¹…åŒ– Cookie å­˜å‚¨
                # ä¸é¢„è®¾ç‰¹å®šç½‘ç«™çš„cookieï¼Œæ”¹ä¸ºåœ¨è®¿é—®é¡µé¢åå¤„ç†cookieåŒæ„
                storage_state={
                    "cookies": []
                }
            )
            
            page = await context.new_page()
            
            # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´
            await page.set_extra_http_headers({
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            })
            
            await page.goto(url, timeout=60000)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await page.wait_for_load_state("domcontentloaded")
            print("é¡µé¢åŠ è½½å®Œæˆ")

            # å°è¯•è‡ªåŠ¨ç‚¹å‡»å„ç§å¸¸è§çš„ Cookie åŒæ„æŒ‰é’®
            try:
                # å¸¸è§çš„ Cookie åŒæ„æŒ‰é’®é€‰æ‹©å™¨åˆ—è¡¨
                cookie_button_selectors = [
                    'button[id="onetrust-accept-btn-handler"]',  # OneTrust
                    'button[id*="cookie-accept"]',            # é€šç”¨å‘½å
                    'button[id*="cookie-consent"]',           # é€šç”¨å‘½å
                    'button[class*="cookie-accept"]',         # é€šç”¨ç±»å
                    'button[class*="cookie-consent"]',        # é€šç”¨ç±»å
                    'button[class*="accept-cookies"]',        # é€šç”¨ç±»å
                    'a[class*="accept-cookies"]',             # é“¾æ¥å½¢å¼
                    'div[class*="cookie-banner"] button',     # é€šç”¨å®¹å™¨å†…æŒ‰é’®
                    'div[id*="cookie-banner"] button',        # é€šç”¨å®¹å™¨å†…æŒ‰é’®
                    'div[class*="cookie-policy"] button',     # é€šç”¨å®¹å™¨å†…æŒ‰é’®
                    'div[id*="gdpr"] button',                 # GDPRç›¸å…³
                    '.cc-accept',                              # å¸¸è§ç±»å
                    '.accept-cookies'                          # å¸¸è§ç±»å
                ]
                
                # å°è¯•ç‚¹å‡»æ¯ä¸€ä¸ªå¯èƒ½çš„æŒ‰é’®
                for selector in cookie_button_selectors:
                    try:
                        accept_button = await page.wait_for_selector(selector, timeout=1000)
                        if accept_button:
                            await accept_button.click()
                            print(f"å·²ç‚¹å‡» Cookie åŒæ„æŒ‰é’®: {selector}")
                            # ç­‰å¾…é¡µé¢é‡æ–°åŠ è½½
                            await page.wait_for_load_state("networkidle", timeout=5000)
                            break
                    except Exception:
                        continue
            except Exception as e:
                print(f"å¤„ç† Cookie åŒæ„æŒ‰é’®æ—¶å‡ºé”™: {e}")
            
            # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
            await page.wait_for_load_state("load")
            print("ä¸»è¦å†…å®¹åŠ è½½å®Œæˆ")

            await asyncio.sleep(5)
            
            content = await page.content()
            
            # è°ƒè¯•ç”¨ï¼šä¿å­˜é¡µé¢æˆªå›¾
            if (not content or "cookie" in content.lower()):
                await page.screenshot(path=SH_PICTURE_FILE)
                print("å·²ä¿å­˜è°ƒè¯•æˆªå›¾åˆ° debug_screenshot.png")
            
            await browser.close()
            return content
            
        except Exception as e:
            print(f"[Playwright æŠ“å–å¤±è´¥] {e}")
            return None


async def multi_cralwer(target_url, save_files=False):
    url_input = target_url.strip()
    if not url_input.startswith(("http://", "https://")):
        print("âŒ æ— æ•ˆçš„ URLï¼Œè¯·ç¡®ä¿ä»¥ http:// æˆ– https:// å¼€å¤´ã€‚")
        return None, None
    real_url = resolve_google_redirect(url_input)
    if real_url != url_input:
        print(f"å·²è§£æçœŸå®åœ°å€: {real_url}")

    # ç¬¬ä¸€é˜¶æ®µï¼šcrawl4ai
    print("ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šå°è¯• crawl4ai æŠ“å–...")
    html = await try_crawl4ai(real_url)
    title, clean_html, md_text = extract_readable_content(html)
    if md_text and len(md_text) > 0:
        await save_result(title, clean_html, md_text, save_files)
        print("âœ… crawl4ai æŠ“å–æˆåŠŸ")
        return md_text, real_url

    # ç¬¬äºŒé˜¶æ®µï¼šPlaywright æ— å¤´æ¨¡å¼
    print("âš ï¸ ç¬¬äºŒé˜¶æ®µï¼šcrawl4ai æŠ“å–å¤±è´¥ï¼Œå°è¯• Playwright æ— å¤´æ¨¡å¼...")
    html = await try_playwright(real_url, headless=True)
    title, clean_html, md_text = extract_readable_content(html)
    if md_text and len(md_text) > 0:
        print("âœ… Playwright æ— å¤´æ¨¡å¼æŠ“å–æˆåŠŸ")
        await save_result(title, clean_html, md_text, save_files)
        return md_text, real_url

    # ç¬¬ä¸‰é˜¶æ®µï¼šå¦‚æœæ— å¤´æ¨¡å¼å¤±è´¥ï¼Œå°è¯•å¯è§æµè§ˆå™¨
    print("âš ï¸ ç¬¬ä¸‰é˜¶æ®µï¼šæ— å¤´æ¨¡å¼å¤±è´¥ï¼Œå¯åŠ¨å¯è§æµè§ˆå™¨ï¼Œè¯·æ‰‹åŠ¨è¿‡éªŒè¯...")
    html = await try_playwright(real_url, headless=False)
    title, clean_html, md_text = extract_readable_content(html)
    if md_text and len(md_text) > 0:
        print("âœ… æ‰‹åŠ¨æ“ä½œåæŠ“å–æˆåŠŸ")
        await save_result(title, clean_html, md_text, save_files)
        return md_text, real_url
    else:
        print("âŒ ä»ç„¶æ— æ³•æŠ“å–è¯¥é¡µé¢")
    
    # å³ä½¿æ‰€æœ‰æŠ“å–æ–¹æ³•éƒ½å¤±è´¥ï¼Œä»ç„¶è¿”å› real_url
    print("âš ï¸ æ‰€æœ‰æŠ“å–æ–¹æ³•éƒ½å¤±è´¥ï¼Œä½†ä»è¿”å›è§£æåçš„ URL")
    return None, real_url


if __name__ == "__main__":
    BASE_DIR = os.path.join(os.path.dirname(__file__), "test")

    url = 'https://www.google.com/url?rct=j&sa=t&url=https://www.binance.com/zh-CN/square/post/08-16-2025-btc-15-66-28404131337498&ct=ga&cd=CAIyIGQ0OGVkZDFmZDIyYTgzMGU6Y29tLmhrOnpoLUNOOkhL&usg=AOvVaw11XXR3l1oZ070UwQ3QtGFi'
    asyncio.run(multi_cralwer(url, save_files=True))