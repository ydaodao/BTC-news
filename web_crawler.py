import asyncio
import aiofiles
import os
from urllib.parse import urlparse, parse_qs
from crawl4ai import AsyncWebCrawler
from playwright.async_api import async_playwright
from readability import Document
import html2text


# === é…ç½® ===
BASE_DIR = os.path.dirname(__file__)
HTML_FILE = os.path.join(BASE_DIR, "test/output.html")
MD_FILE = os.path.join(BASE_DIR, "test/output.md")
SH_PICTURE_FILE = os.path.join(BASE_DIR, "test/debug_screenshot.png")
WAIT_TIME = 15  # å¯è§†åŒ–æ¨¡å¼æ‰‹åŠ¨æ“ä½œç­‰å¾…æ—¶é—´
# VPN ä»£ç†é…ç½®
PROXIES = {
    'http': 'http://127.0.0.1:7890',
    'https': 'http://127.0.0.1:7890'
}


def resolve_google_redirect(url):
    """è§£æ Google è·³è½¬ URL"""
    parsed = urlparse(url)
    if "google.com" in parsed.netloc and "url" in parsed.path:
        query = parse_qs(parsed.query)
        if "url" in query:
            return query["url"][0]
    return url


def is_challenge_page(html):
    """åˆ¤æ–­æ˜¯å¦æ˜¯åçˆ¬/éªŒè¯ç é¡µé¢"""
    return any(
        keyword in html.lower()
        for keyword in ["javascript is disabled", "captcha", "cloudflare", "awswaf"]
    )


def extract_readable_content(html):
    """ç”¨ Readability æå–æ­£æ–‡å¹¶è½¬ Markdown"""
    doc = Document(html)
    title = doc.short_title()
    content_html = doc.summary(html_partial=True)

    md_converter = html2text.HTML2Text()
    md_converter.ignore_links = False
    md_text = md_converter.handle(content_html)
    return title, content_html, md_text


async def save_result(title, html, md_text, save_files=False):
    if not save_files:
        return
    """ä¿å­˜ HTML å’Œ Markdown"""
    async with aiofiles.open(HTML_FILE, "w", encoding="utf-8") as f:
        await f.write(html)
    async with aiofiles.open(MD_FILE, "w", encoding="utf-8") as f:
        await f.write(f"# {title}\n\n{md_text}")
    print(f"âœ… HTML ä¿å­˜åˆ°: {HTML_FILE}")
    print(f"âœ… Markdown ä¿å­˜åˆ°: {MD_FILE}")


async def try_crawl4ai(url):
    """ç¬¬ä¸€é˜¶æ®µï¼šç›´æ¥ç”¨ crawl4ai æŠ“å–"""
    try:
        async with AsyncWebCrawler(proxy=PROXIES['http'], max_concurrency=5) as crawler:
            result = await crawler.arun(url=url)
            if result and result.html and not is_challenge_page(result.html):
                return result.html
    except Exception as e:
        print(f"[crawl4ai æŠ“å–å¤±è´¥] {e}")
    return None


async def try_playwright(url, headless=True, debug_mode=False):
    """ç¬¬äºŒé˜¶æ®µï¼šä½¿ç”¨ Playwright æŠ“å–"""
    async with async_playwright() as p:
        try:
            browser = await p.chromium.launch(
                headless=headless, 
                slow_mo=100,
                proxy={
                    "server": PROXIES['http'],
                }
            )
            
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
                viewport={"width": 1920, "height": 1080},
                # æ·»åŠ æŒä¹…åŒ– Cookie å­˜å‚¨
                storage_state={
                    "cookies": [
                        {
                            "name": "cookie_consent",
                            "value": "accepted",
                            "domain": ".binance.com",
                            "path": "/"
                        }
                    ]
                }
            )
            
            page = await context.new_page()
            
            # æ·»åŠ æ›´å¤šè¯·æ±‚å¤´
            await page.set_extra_http_headers({
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            })
            
            await page.goto(url, timeout=60000)
            
            # ç­‰å¾…é¡µé¢åŠ è½½
            await page.wait_for_load_state("domcontentloaded")
            
            # å°è¯•è‡ªåŠ¨ç‚¹å‡» Cookie åŒæ„æŒ‰é’®
            try:
                # ç­‰å¾…å¹¶ç‚¹å‡» Cookie åŒæ„æŒ‰é’®ï¼ˆæ ¹æ®å®é™…æŒ‰é’®é€‰æ‹©å™¨è°ƒæ•´ï¼‰
                accept_button = await page.wait_for_selector('button[id="onetrust-accept-btn-handler"]', timeout=5000)
                if accept_button:
                    await accept_button.click()
                    # ç­‰å¾…é¡µé¢é‡æ–°åŠ è½½
                    await page.wait_for_load_state("networkidle")
            except Exception:
                print("æœªæ‰¾åˆ° Cookie åŒæ„æŒ‰é’®æˆ–å·²ç»åŒæ„")
            
            # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
            await page.wait_for_load_state("load")
            await asyncio.sleep(5)
            
            content = await page.content()
            
            # è°ƒè¯•ç”¨ï¼šä¿å­˜é¡µé¢æˆªå›¾
            if debug_mode and (not content or "cookie" in content.lower()):
                await page.screenshot(path=SH_PICTURE_FILE)
                print("å·²ä¿å­˜è°ƒè¯•æˆªå›¾åˆ° debug_screenshot.png")
            
            await browser.close()
            return content
            
        except Exception as e:
            print(f"[Playwright æŠ“å–å¤±è´¥] {e}")
            return None


async def multi_cralwer(target_url, save_files=False, debug_mode=True):
    url_input = target_url.strip()
    if not url_input.startswith(("http://", "https://")):
        print("âŒ æ— æ•ˆçš„ URLï¼Œè¯·ç¡®ä¿ä»¥ http:// æˆ– https:// å¼€å¤´ã€‚")
        return
    real_url = resolve_google_redirect(url_input)
    if real_url != url_input:
        print(f"å·²è§£æçœŸå®åœ°å€: {real_url}")

    # ç¬¬ä¸€é˜¶æ®µï¼šcrawl4ai
    print("ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šå°è¯• crawl4ai æŠ“å–...")
    html = await try_crawl4ai(real_url)
    if html:
        print("âœ… crawl4ai æŠ“å–æˆåŠŸ")
        title, clean_html, md_text = extract_readable_content(html)
        await save_result(title, clean_html, md_text, save_files)
        return md_text, real_url

    # ç¬¬äºŒé˜¶æ®µï¼šPlaywright æ— å¤´æ¨¡å¼
    print("âš ï¸ ç¬¬äºŒé˜¶æ®µï¼šcrawl4ai æŠ“å–å¤±è´¥ï¼Œå°è¯• Playwright æ— å¤´æ¨¡å¼...")
    html = await try_playwright(real_url, headless=True, debug_mode=debug_mode)
    if html:
        print("âœ… Playwright æ— å¤´æ¨¡å¼æŠ“å–æˆåŠŸ")
        title, clean_html, md_text = extract_readable_content(html)
        await save_result(title, clean_html, md_text, save_files)
        return md_text, real_url

    # ç¬¬ä¸‰é˜¶æ®µï¼šå¦‚æœæ— å¤´æ¨¡å¼å¤±è´¥ï¼Œå°è¯•å¯è§æµè§ˆå™¨
    if debug_mode:
        print("âš ï¸ ç¬¬ä¸‰é˜¶æ®µï¼šæ— å¤´æ¨¡å¼å¤±è´¥ï¼Œå¯åŠ¨å¯è§æµè§ˆå™¨ï¼Œè¯·æ‰‹åŠ¨è¿‡éªŒè¯...")
        html = await try_playwright(real_url, headless=False, debug_mode=debug_mode)
        if html:
            print("âœ… æ‰‹åŠ¨æ“ä½œåæŠ“å–æˆåŠŸ")
            title, clean_html, md_text = extract_readable_content(html)
            await save_result(title, clean_html, md_text, save_files)
            return md_text, real_url
        else:
            print("âŒ ä»ç„¶æ— æ³•æŠ“å–è¯¥é¡µé¢")


if __name__ == "__main__":
    BASE_DIR = os.path.join(os.path.dirname(__file__), "test")

    url = 'https://www.google.com/url?rct=j&sa=t&url=https://www.fx168news.com/article/%25E6%25AF%2594%25E7%2589%25B9%25E5%25B8%2581-930071&ct=ga&cd=CAIyIGQ0OGVkZDFmZDIyYTgzMGU6Y29tLmhrOnpoLUNOOkhL&usg=AOvVaw1pY9U2l1FepoUqeYekeLDE'
    asyncio.run(multi_cralwer(url, save_files=True, debug_mode=True))